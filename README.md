E-learning websites like Coursera, Udemy, edX, and Khan Academy are making education more attainable by overcoming constraints related to location and finances. These platforms provide an extensive range of courses covering various subjects, such as business, technology, the arts, and the humanities, to satisfy the demands of a wide range of people.  The flexibility to learn at one's own pace and the availability of high-quality resources attract millions of learners worldwide. To administer these learning platforms, different forms of surveys have been devised with the aim of personalizing the learning experience and providing the course developer with information that can potentially contribute to the improvement of both the learning environment and the quality of the study content. Owing to the fact that sentiment expressed in the surveys varies from student to student and course to course, it becomes important to formulate a pipeline that can assist educational institutions and stakeholders in getting deeper insights on the survey data and respective course performance correlated with the reviews and ratings provided by learners. Towards this objective, we have employed explainable artificial intelligence (XAI) frameworks, large language models, and generative AI in order to precisely analyze and interpret the diverse feedback received from students. 

Objective: The primary objective is to develop a sophisticated machine learning pipeline that can provide educational institutions and stakeholders with a more profound understanding of course performance and learner feedback through advanced AI-driven analysis. By analyzing course feedback, we can subsequently enhance the learning environment and improve the quality of study content on e-learning platforms. This pipeline integrates web scraping, language model classification, understandable AI, and a chatbot to provide actionable insights for improving course quality and learning environments. 

Research Questions: We formulate this objective through three research questions, which are outlined below. Our research to address these questions constitutes the contribution of this thesis.

RQ1: What are the comparative performance and effectiveness of different language model architectures (GRU, BiLSTM, and BERT) in classifying learner feedback from learning engagement platforms?
RQ2: How can we use explainable AI frameworks like SHAP and Transformer Interpret to improve transparency and confidence in language predictions?
RQ3: What are the potential benefits of using generative AI tools, such as custom-trained chatbots leveraging large language models, to create user-friendly interfaces that simplify the interpretation of complex feedback data?
